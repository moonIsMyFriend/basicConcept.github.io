---
layout: post
read_time: true
show_date: true
title:  Reinforcement Learning
date:   2024-10-01
description: Planning
img: posts/20210324/starting_adventure.jpg
tags: [RL]
author: 
github:  
mathjax: yes
---


1. 감가율 discount factor 나중에 받을 수록 보상의 가치를 줄이는 것 

2-1.가치함수 어떤 상태에 있으면 얼마의 보상을 받을 것인지에 대한 기대값. 에이전트가 가지고 있는 값으로 직접 다 경험하지 않더라도 보상을 예상. 앞으로 받을 보상은 정책에 따라서 계산되어야함. 

> 벨만 기대 방정식: 계산을 하려면 환경 모델의 확률과 보상을 알아야한다. 

2-2. 행동 가치함수( QFunction): 어떤 상태에서 어떤 행동이 얼마나 좋은지 알려줌


3-1. 순차적 행동 결정 문제  
3-2.. 다이내믹 프로그래밍 (1953 by 벨만)
큰 문제 안에 작은 문제들이 중첩된 경우  전체 큰 문제를 작은 문제로 쪼개서 풀겠다. 단점 계산을 빠르게 하는 것이지 학습을 하지 않는다. 순차적 문제를 벨만방정식으로 푸는 것.  

단점 3가지
1. 계산 복잡도 = 상태 크기의 3제곱
2. 차원의 저주.
3. 환경에 대한 롼벽한 정보가 필요.
환경을 모르지만 환경과 상호작용을 통해 경험을 바탕으로 학습하는 강화학습 등장.

3-2-1.정책 이터레이션은 벨만 기대 방정식을 이용하여 주변 상태의 가치함수와 한 타임스텝의 보상만 고려해서 현재 상태의 다음 가치함수를 계산하여 정책을 평가. 여러번 반복하여 실제 값에 수럼.
정책의.발전은 큐함수를 텅해 액션을 선택(탐욕 정책 발전 Greedy policy improvement). 명시적인(explicit) 정책이 있으며, 그 정책을 평가하는 도구로서 가치함수를 이용. 정책과 가치함수가 분리되어 정책이 독립적이므로(결정적이 아니라 어떤 정책도 가능) 벨만 기대 방 이용. 

3-2-2. 가치 이터레이션은 벨만 최적방정식 이용. 명시적인 정책이 아닌 가치함수 내에 내재적으로 표현. 시작부터 최적정책을 가정했기 때문에 정책 발전은 필요없고 한 번의 전책 평가로 최적 가치함수와 최적 정책이 구해짐.

정책과 가치 이터레이션은 살사 > off policy 로 변형 > 큐러닝으로 발전

모델링: 입력과 출력의 관계를 나타내는 과정. 정확할수록 복잡하며 정확하게 모델링한느 것은 불가능에 가깝다.


________
큐러닝  
강화와 다이내의 차이 환경 모델을 몰라도 끊임없는 경험 반복으로 학습.  
예측: 경험으로 주어진 정책에 대한 가치함수를 학습. 모테카를로 예측과 시간차 예측  
제어: 가치함수를 토대로 정책을 발전시켜 최적 정책을 학습. 살사 > 큐러닝(오프폴리시 제어)  

고전 알고리즘이지만 수많은 강화 알고리즘의 토대. 차이와 한계를 이해 중요. 

몬테카를로: 에피소드 동안 방문핶던 모든 상태의 가치함수를 업뎃하고 에이전트는 다시 시작 상태에서부터 새로운 에피소드 진행. 단점 실시간이 아님. 에피소드가 끝나야 가차함수를 업뎃. 

시간차 예측: G대신 rV(s')로 가치함수 정의하고. 실시간으로 업뎃. 몬테보다 효율적이고 빠름.  
부트스트랩: 다른 상태의 가치함수 예측값을 통해 지금 상태의 가치함수를 예측하는 방식. 업뎃 목표도 정확하지 않은 상황에서 가치함수를 업뎃. 
여기까지가 GPI 의 정책 평가 과정을 수행. 새로운 정책을 얻지만 현재 상태에 대해서만 업뎃. 모든 상태의 정책으로 발전 못함. 하지만 시간차 방법에서는 탐욕 정책으로 발전가능.  
시간차 예측 + 탐욕정책 = 시간차 제어  
살사: 살사부터 강화학습이라 부름. 시간차 제어. 큐함수를 토대로 샘플을 엡실론 탐욕 정책으로 모으고 큐함수를 업뎃 반복. 132. 살사는 GPI의 정책 평가를 큐함수를 이용한 시간차 예측으로, 탐욕 정책 발전을 ${epcilon}$-탐욕정책으로 변화시킨 강화학습 알고리즘. 또한 정책 이터레이션과는 달리 별도의 정책 없이 엡실론-탐욕 정책을 사용하는 것은 가치 이터레이션에서 개념을을 가져옴.
 
살사: 입실론 3 탐욕정책 사용. 온촐리시 시간차 제어, 즉 자신이 행동하는 대로 학습하는 시간차 제어.따로 정책이 존재하지 않으며 단지 현재 큐함수에 다라 행동. 큐함수 업뎃을 위해 벨만 기대 방정식 사용.

큐러님(1989 chris watkins): 오프폴리시 시간차 제어로 현재 행동하는 정책과 독립적읋 학습. 행동하는 정책으로 지속적인 탐험을 하면서 따로 목표 정책을 둬서 학습은 목표 정책을 따름.큐함수 업뎃을 위해 벨만 최적 장상식 사용. 행동 선택은 입실론 탐욕 정책으로, 럽뎃은 벨만 최적 방정식 이용.

다이내믹과는 다르게 몬테, 살사, 큐러닝은 모델 프리로서 환경에 대한 모델 없이 샘퓰링을 통해 학습. 하지만 상태의 수가 많아지고 변한다면, 큐러닝까지는 상태가 적은 문제만 적용 가능.
_________
근사 함수  
큐함수를 매개변수로 근사
기존의 데이터를 매개변수를 통해 근사하는 함수를 근사함수하고 함.
금사함수는 여러 가지이지만 인공신경망을 사용.

딥살사: 살사 알고리즘 사용하되 큐함수를 인공 신경망으로 구성.
하나의 큐함수값을 업데이트하지 않고 오차함수를 정의하고 경사하강법을 이용하여