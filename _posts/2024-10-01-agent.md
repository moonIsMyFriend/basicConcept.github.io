---
layout: post
read_time: true
show_date: true
title:  Agent
date:   2024-10-01
description: Planning
img: posts/20210318/TicTacToeSml.jpg
tags: [RL]
author: 
github:  
mathjax: yes
---

### 1. 가치 기반(value-based) Agent
- 명시적 정책(explicit policy)가 따로 없고 가치함수 v나 q를 뉴럴넷으로 표현하는 방식(가시함수를 마치 정책처럼 사용해서 내재된 정책(implicit policy)라고 함)
- 모델 프리 상황에서는 V만으로 액션을 정할 수 없으므로 가치 함수 q(s,a)에 근거하여 액션을 선택
- 정책 함수는 불필요
- 결정론적(deterministic): 모든 상태에 대해 각 상태에서 선택하는 액션이 변하지 않음. 학습이 끝나면 Q(s,a)의 값이 고정되기 때문.
- SARSA, Q러닝

### 1.1. 밸류 네트워크
$L(\theta) = E_\pi \left[ (v_{true}(s) - v_\theta (s))^2 \right]$ 
- 손실함수$L(\theta)$: 정책이 자주 방문하는 상태의 가중치는 높이고, 드물게 방문하는 상태의 가중치는 낮아짐. 결국, 중요한 상태의 밸류를 더 정확하게 계산하여 뉴럴넷을 학습.  
- 기대값 $E_\pi$은 정책 함수를 이용해 방문했던 상태에 대해 예측과 정답 사이의 차이를 계산하라는 의미.
- 밸류 네트워크 $v_\theta (s)$: 뉴럴넷으로 이루어진 가치 함수 
- $\theta$ 는 뉴럴넷의 파라미터 벡터
- 스텝 사이즈: $\theta$를 한 번에 얼마큼 갱신할지 결정

#### 1.1.1. $L(\theta)$의 $\theta$에 대한 그라디언트
$\nabla L(\theta) \approx  -(v_{true}(s)-v_{\theta}(s)){\nabla}_ {\theta} v_{\theta} (s)$  
- 손실함수에서 체인 룰을 이용하여 식을 유도하고 샘플링을 반복하면 위와 같아짐.

#### 1.1.2. $\theta$ 업데이트
$\theta' = \theta - a\nabla_{\theta} L(\theta)$  
$\theta' = \theta + a(v_{true}(s)-v_{\theta}(s)){\nabla}_ {\theta} v_{\theta} (s)$

#### 1.1.3. $v_{true}(s)$ 없이 손실함수 정의하는 법
(1) 몬테카를로 리턴  
- 가치 함수의 정의가 곤 $G_t$이기 때문에 $v_{true}$ 대신에 G를 사용하여 손실 함수 정의  
$L(\theta) = E_\pi \left[ (G_t - v_\theta (s_t))^2 \right]$


- $\theta$ 업데이트  
$\theta' = \theta + a(G_t - v_{\theta}(s_t))\nabla _ \theta v_{\theta}(s_t)$

(2) TD 타깃
- 한 스텝 더 진행해서 추측한 값을 이용하여 현재의 추측치를 업데이트하는 방식. TD 타깃을 대입하여 구함.  
$L(\theta) = E_\pi \left[ (r_{t+1} + \gamma v_{\theta|(s_{t+1})} - v_\theta (s_t))^2 \right]$

- $\theta$ 업데이트  
$\theta' = \theta + a((r_{t+1} + \gamma v_{\theta|(s_{t+1})} - v_{\theta}(s_t))\nabla _ \theta v_{\theta}(s_t)$

- TD 타깃은 변수가 아니라 상수 취급. 업데이트 시점의 $\theta$ 를 이용하여 편미분시 0이 될 수 있고, 목적지를 변하지 않게 하기 위함.
상수 취급하면 내가 목적지를 향해 가는 것이고 변수 취급하면 목적지를 향해서 가지만 목적지도 나를 향해서 다가오게 됨.(코딩시에는 detach함수 이용)

### 1.2. 딥 Q-러닝
- $r+\gamma \max_a q_* (s',a')$를 정답으로 보고 이와 Q의 차이로 손실 함수 정의  
$L(\theta) = E[(r+\gamma \max_a Q_{\theta} (s',a')-Q_{\theta}(s,a))^2]$

- 같은 s와 a를 선택해도 다른 상태에 도달할 수 있기 때문에 손실 함수를 정의할 때 기대값이 필요하다.
하지만 뉴럴넷에서는 샘플 기반 방법론으로 기대값 연산을 무시하고 계산함. 데이터를 여러 개 모아서 평균을 구해서 업데이트. 
$\theta' = \theta + a(r+\gamma \max_a Q_{\theta} (s',a')-Q_{\theta}(s,a)) \nabla_{\theta} Q_{\theta}(s,a)$

- off-policy: 실제 행동 선택 정책은 $\epsilon -greedy\,  Q_{\theta}$ 이고, 학습용 타깃은 $greedy\, Q_{\theta}$로 서로 다름

- 미니 배치: 기대값 연산자를 없애기 위해 여러 개의 샘플을 뽑는 것.

### 1.3. DQN
뉴럴넷을 이용한 Q함수
> human-level control through deep reinforcement learning (2015)

#### 1.3.1. Experience Replay
- 경험을 재사용하여 강화 학습
- 경험은 여러 개의 에피소드로 이루어져 있고, 에피소드는 여러 개의 상태 전이(state transition)로 이루어짐.
- replay buffer: 낱개의 데이터를 재사용하기 위한 버퍼로 가장 최근의 데이터 n개만 저장.
- 학습할 때는 이 버퍼에서 램덤하게 데이터를 뽑아서 여러번 재사용이 가능.
- 하나의 미니 배치안에 서로 다른 게임에서 발생한 다양한 데이터들이 섞여서 한 게임에서만 연속된 데이터를 사용한 경우보다 각각의 데이터 간의 상관성이 작아서 성능 개선에 도움.
- 주의: off-policy에만 사용 가능. 버퍼에 담은 데이터는 현재 학습을 받고 있는 정책이 아니라 과거의 정책이 생성한 데이터임. 즉, 과거의 경험으로 현재의 내가 배우고 있음.

#### 1.3.2. 별도의 Target Network
- $r+\gamma \max_a q_* (s',a')$를 정답으로 사용하므로 정답이 $\theta$에 의존적이어서 $\theta$가 갱신될 때 마다 정답도 변하여 학습에 악영향.
- 따라서, 정답 계산용 네트워크인 타깃 네트워크와 학습을 받고 있는 Q네트워크로 분리하여 정답 계산용 네트워크의 파라미터를 얼려두었다가 일정 주기마다 최신 파라미터로 교체.



### 2. 정책 기반(policy-based) Agent
정책 함수 $ \pi (a | s) $를 뉴럴넷으로 표현하는 방식. 
- 정책함수를 보고 직접 액션을 선택하여 경험해나가면서 정책을 강화.
- 확률적 정책(연속적인 공간에서도 정책기반으로 액션을 선택할 수 있기 때문에 유연한 정책)

#### 2.1. 목적 함수
- 손실 함수에서 필요한 정책 함수의 정답은 최적 정책 하나인데 이를 알면 강화 학습을 하는 이유가 없음. 따라서, 정책을 평가하는 함수를 만들어서 그 값을 증가시키도록 그라디언트 어센트를 함.
- 리턴의 기대값은 가치 함수이므로 아래의 식 유도( d(s)는 어떤 상태 s에서 시작할 확률)  
$J(\theta) = \mathbb{E}_ {\pi_ \theta} \left[  \sum\limits_{t} r_t \right]  = v_{\pi_ \theta} (s)$  

- 그라디언 디센트  
$\theta' = \theta + a * \nabla_{\theta} J(\theta)$

#### 2.2. 1-step MDP
- 한 스텝만 진행하고 바로 에피소드가 끝나는 MDP
- 보상과 확률을 모르는 모델 프리 상황이므로 샘플 기반 방법론을 적용하여 아래식 도출  
$ \nabla_ \theta J(\theta) =  \mathbb{E}_ {\pi_ \theta} [ \nabla_{\theta} \log \pi_ {\theta} (s,a) * R_{s,a} ]$
- $V_\theta\log \pi_ {\theta} (s,a) $는 뉴럴넷의 그라디언트
- $R_{s,a} $는 상태전이를 여러 번 하여 보상을 관측하여 구함 

#### 2.3. 일반 MDP의 정책 기반(policy gradient theorem)
- 보상 대신 리턴의 기대값을 사용. 한 스텝으로 끝이 아니라 이후 스텝에서 받을 보상까지 더해준 개념  
$ \nabla_ \theta J(\theta) =  \mathbb{E}_ {\pi_ \theta} [ \nabla_{\theta} \log \pi_ {\theta} (s,a) * Q_{\pi_ \theta}(s,a) ]$

### 2.4.. REINFORCE 알고리즘
$ \nabla_ \theta J(\theta) =  \mathbb{E}_ {\pi_ \theta} [ \nabla_{\theta}  \log \pi_ {\theta} (s,a) * G_t ]$
- G의 샘플을 여러 개 얻어서 평균을 내면 그 값은 실제 액션-밸류인 Q(s,a)에 근사해지므로 그라디언트 식에 이용.

$\theta' = \theta + a * \nabla_{\theta} J(\theta)$  
$\theta' = \theta + a * \nabla_{\theta} \log \pi_ {\theta} (s,a) * G_t$
- G가 1이면 액션의 확률을 증가, -1인 액션의 확률은 감소시켜 좋은 액션을 강화시킴.



### 3. 액터-크리틱
- 가치 함수와 정책 함수 모두 사용.
- 액터는 행동가, 즉 정책을 의미하고 크리틱은 비평가. 가치함수(v(s) 또는 q(s,a))를 지칭
- $\pi_ {\theta}$는 실행할 액션 a를 선택하는 actor 역할 담당. Q의 평가를 통해 결과가 좋으면 강화 반대면 약화하는 방식으로 학습.
- $Q_w$는 선택된 액션의 밸류를 평가하는 critic 역할. 현재의 정책 함수의 가치를 학습하는 방향으로 업데이트.

### 3.1 Q 액터-크리틱
- policy gradient식에서 $Q_{\pi_ \theta} (s,a)$는 미지의 함수이어서 w로 파라미터화 된 뉴럴넷$Q_w (s,a) \approx Q_{\pi_ \theta} (s,a)$를 도입.

### 어드벤티지 액터-크리틱
- policy gradient식에서 상태의 밸류가 높은 액션을 모두 강화하는 것은 비효율적이다.
- 어드벤티지 $A_{\pi_ \theta} (s,a) $: $Q_{\pi_ \theta} (s,a) - V_{\pi_ \theta} (s)$를 적용하여 이미 일어난 상태s에 있는 것보다 액션 a를 실행함으로써 추가로 얼마의 가치를 더 얻을지에 대한 값. 이를 곱해줌으로써 분산이 줄어들어 추정치의 변동성이 작아지고 안정적인 학습이 가능해짐.
- 정책 함수 뉴럴넷($\theta$), 액션 가치 함수의 뉴럴넷 (w), 가치 함수의 뉴럴넷 ($\phi$)s 가 필요.

### TD 액터-크리틱
- 어브벤티지 액터-크리틱은 3쌍의 뉴럴넷이 필요하다는 단점이 존재.
- TD 에러인 $\delta$의 기대값이 어드벤티지 A(s,a)에 수렴한다. 
- 즉, $\delta$ 은 A의 불편 추정량.  
$\delta = r + \gamma V(s') -V(s) $
- TD 에러 $\delta$ 를 어브밴티지 A 대신에 사용.





