---
layout: post
read_time: true
show_date: true
title:  Agent
date:   2024-10-01
description: Planning
img: posts/20210318/TicTacToeSml.jpg
tags: [RL]
author: 
github:  
mathjax: yes
---

### 1. 가치 기반(value-based) Agent
- 모델 프리 상황에서 V만으로 액션을 정할 수 없으므로 가치 함수 q(s,a)에 근거하여 액션을 선택
- 정책 함수는 불필요
- 결정론전(deterministic): 모든 상태에 대해 각 상태에서 선택하는 액션이 변하지 않음. 학습이 끝나면 Q(s,a)의 값이 고정되기 때문.
- SARSA, Q러닝

### 1.1. 밸류 네트워크
$L(\theta) = E_\pi \left[ (v_{true}(s) - v_\theta (s))^2 \right]$ 
- 손실함수$L(\theta)$: 정책이 자주 방문하는 상태의 가중치는 높이고, 드물게 방문하는 상태의 가중치는 낮아짐. 결국, 중요한 상태의 밸류를 더 정확하게 계산함.  
- 밸류 네트워크 $v_\theta (s)$: 뉴럴넷으로 이루어진 가치 함수 
- $\theta$ 는 뉴럴넷의 파라미터  
- 기대값 $E_\pi$은 정책 함수를 이용해 방문했던 상태에 대해 예측돠 정답 사이의 차이를 계산하라는 의미.
- 스텝 사이즈: $\theta$를 한 번에 얼마큼 갱신할지 결정

#### $v_{true}(s)$ 없이 손실함수 정의하는 법
(1) 몬테카를로 리턴  
- 손실 함수 정의  
$L(\theta) = E_\pi \left[ (G_t - v_\theta (s_t))^2 \right]$  

- $\theta$ 업데이트  
$\theta' = \theta + a(G_t - v_{\theta}(s_t))\nabla _ \theta v_{\theta}(s_t)$

(2) TD 타깃

### 1.2. 딥 Q-러닝

미니 배치:

#### Experience Replay

#### Target Network


### 2. 정책 기반(policy-based) Agent
정책 함수 $\pi(a|s)$를 보고 직접 액션을 선택하여 경험해나가면서 정책을 강화.
- 확률적 정책(유연한) 
#### 목적 함수
#### 1-step MDP
#### 일반 MDP의 정책 기반
### 2.2. REINFORCE 알고리즘


### 3. 액터-크리틱
- 가치 함수와 정책 함수 모두 사용.
- 액터는 행동가, 즉 정책을 의미하고 크리틱은 비평가 가치함수(v(s) 또는 q(s,a))를 지칭
### Q 액터-크리틱
### 어드벤티지 액터-크리틱
### TD 액터-크리틱

