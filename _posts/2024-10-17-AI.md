---
layout: post
read_time: true
show_date: true
title:  AI
date:   2024-10-01
description: Planning
img: posts/20210324/starting_adventure.jpg
tags: [RL]
author: 
github:  
mathjax: yes
---

novel: 새로운
fidelity: 충실함, 동등
rationality: 합리성
sufficient: 구현 가능한, 충분한
perceptual: 지각있는
perceive: 인지하다
inference: 추론
priori: 선험적인
vast: 방대한
nondeterministic: 확률값 없이 예측
deliberating: 계획적인, 신중한
utility function: 효용함수
derive: 끌어내다, 유래하다
-------- 3 ----------------
anomaly: 이상치

supervised
unsupervised
reinforcement learning
exclusive: 배타적인
ambiguous: 모호한
denote: 의미하다
posteriori: 사후
 arguably: 틀림없이
 typical: 대표적인
 nontrivial: 중요한, 자명한
 latent: 숨어있는, 잠재된
 infer: 결론을 도출하다, 추론하다
 reduction: 축소
 variability: 가변성
 ICA(Independent Component Analysis):
 PCA:
 empirical: 
 indicator function:
 curse: 저주
 residual error: 선형예측과 실제값의 계산상 오차
 
 다양한 예에 적용하기위해 비선형 모델을 이용
가우시안 분포
베르누이 분포
e.g. : 예를 들어
c.f. : 비교하다
overfit
KNN: K를 증가시켜서 훈련 se에서 오차율을 증가시킴 38
cross validation

--------------- 4 ----------------
r.v. (random variables)
for short: 생략하여
pmf: 확률질량함수
배타적
독립적
joint probabilities: 결합확률
marginal distribution: 주변확률분포. 결합분포에서 원하는 하나의 확률변수를 선택하고, 나머지 확률변수에 대해서는 모든 가능한 값들의 합 또는 적분을 통해 구함
arbitrary: 임의의
discriminative classifier 판별형 분류기
CI(conditionalyy independent)
cdf(cumulative disribution function) 누적 분포 함수
monotonically: 단조롭게, 꾸준히
pdf: probability density function
inverse F: 역함수
최소자승법의 위험을 방지하고자 lasso이용
시그마
deviation: 편차

----------------- 5-------------------------

sigmoid: 0과 1사이로 변환. 증가 함수
tanh: hyperbolic tangent) -1과 1 사이로 변환. 증가함수
Relu: rectified linear activation. 0 이상. 증가함수. 일부만 활성화시켜서 출력가능. 미분을 해야할 때 식을 간단하게 해줌.
오메가:
empirical 경합할 수 있는
regularizer
surrogate: 대리
SGD(stochastic gradient descent)
epoch
log-likelihood
nature log
cross-entropy
chain ----------
L2 regularization ridge(square) RMSE 크기를 줄이는 것이 목적. 웨잇 디케잉
L1 : lasso 절대값(absolute) MAE 대부분을 0으로 반드는 것이 목적. 스팔스

-----------------------6 ----------------------
parameter sharing
pooling: 옆으로 이동시 max값은 변하지 않을 확률이 높음.
subsampling
local connectivity
cnn:convolutional and pooling layers 번갈아 넣음
softmax: 둘 이상의 클래스가 존재하는 분류문제에 사용

--------------7 ----------------------------
rnn: scale to much longer sequences
shallow: 얕은, 피상적인
unfolding
backpropagation
gated uints: vanishing gradients, exploding gradient 를 막아줌(derivaative)으로써 가중치 조정(accumulation or forgetting)
LSTM(long short-term memory):
GRUs(gated recurrent units)

------------------8------------------------
RL: 특정 상황에서 정답을 주지 않음
i.i.d. : independent and identically distributed
markov state:
POMDP(Partially observable Markov decision process)
policy: the agent's behavior
Value function: 최적ㅇ 정책을 따랐을 때 최적의 가치함수, 특정 정책에 대한 가치함수(전체 state에 대한 가치)
model: predict what environment will do next(각 state 마다 보상)
exploitation: go to your favorite restaurant
exploration: try a new restaurant
prediction: evaluate the future given a policy
control: optimaize the future (find the best policy)

-------------- 1028 -----------------------------
regression 문제: 예측하려는 값이 연속적인 값인 경우
multiple regression: 둘 이상의 feature를 이용해서 값을 예측
univariate regression: 하나의 값만 예측
regression의 성능척도
- RMSE: root means squared error
- MAE: mean absolute error

train/test를 나눠서 범용적인 모델을 만들기 위함.단순히 무작위로 인스턴스를 나누게 되면 원 데이터가 갑고 있는 어떤 데이터 분포가 train/test set을 나누는 과정에서 왜곡될 수 있다. 따라서 무작위로 선택을 하되 분포정보가 왜곡되지 않도록 해야한다.
계층적 샘풀링: 원 데이터의 중위소득의 분포가 테스트 데이터에서도 가능한 그대로 유지하게끔 하는 것이 중요.
cleaning 작업: 비정상 데이터는 제거하거나 다른 값으로 채워넣는 방식
imputer:
one-hot encoding: 카테고리형 변수를 벡터로 변환하기 위해 카테고리 수의 크기를 갖는 벡터를 반든 다음, 해당 카테고리에 해당하는 컴포넌트에는 1을 갖고 나머지에는 0을 갖는다.
성능향성을 위해 수치를 스케일링 작업 필요.
min-max 스케일링: nomalization 0 to 1
standardization(표준화) 스케일링: 평균값으로 변환
preprocessing: 전처리(data cleaningrhk scaling 작업)
linear regression
decision tree regression
cross-validation
random forest
grid search: 순차적으로 입력 > 좋은 하이퍼파라미터 찾기

----------------------------- 1104 -------------------------
interpolation: 보간
K-fold cross-validation: 선택한 분류기(kogisticRegression)가 적합한 모델인지 판단
데이터의 불균형을 고려하기 위해 사용
confusion matrix: 에러의 양상을 구체적으로 파악하기 위해서는 분류기가 어떤 입력을 어떤 방식으로 틀리는지 파악하기 위해 혼돈행렬을 이용
accuracy(정확도)
precision(정밀도): 참이라고 말한 것중에 실제로 맞춘 것: tp/(tp+fp)
recall(재현율): 실제로 참인 것중 몇개를 맞춘것 : tp/(tp+fn)
F1-score: 성능 척도 precision과 recall의 조화 평균. 2 x (pr x re)/(pr + re)
ROC Curve: receiver operating characteristic curve tp rate과 fp rate를 동시게 시각화
AUC: area under the curve
Random forest, Naive Bayes는 둘 이상의  class가 존재하는 분류뮨제에 적용 가능 모델
SVM, Linear는 이진 클래스의 분류에만 적용 가능.
one versus all(ovA) = one-versus-the-rest
one versus one(ovo): 두 클래스를 분류하는 분류기를 클래스의 수 x (클래스의 수 -1)/2 개를 학습하고 duel에서 가장 많이 승리한 class를 선택
multilabel classification: 각 입력이 둘 이상의 클래스를 갖는 것을 가정. 성능평가는 f1-score이용(각 label은 0-1 형태)
multioutput classification은 멀티라벨보다 일반ㄴ적인 문제로 label이 multi-class인 경우

------------------------ 1111 ------------------------------------------
선형회귀: 학습방법(오차를 최소화하는 가중치를 찾는 것이 목표)
- direct "closed-form" equation
- iterative optimization approach: normal equation을 사용할 경우 계산복잡도가 실용적이지 아니어서 반복기법 사용.
Gradient Descent: cost function이 감소하는 방향으로 위치를ㄹ 변경하면서 최소가 되는 위치를 찾기 위해 적용.
learning step이 너무 작으면 수렴속도가 느려짐. 크면 지그재그 현상 살생.
batch gradient descent: 파라미터를 한 번 업데이트 하기 위해 전체 데이터를 이용. 데이터가 크면 계산량도 커짐.
stochastic gradient descent: 무작위로 샘플링한 하나의 데이터에 의한 에러함수의 gradient를 이용. 변동성이 커서 learning step 설정시 주의가 필요
mini-batch gradient descent: 미리 정해진 개수의 데이터를ㄹ 무작위로 선택해서 gradient를 계산
polynomial regression: 특수한 관계 외에 (비선형 관계)적용하기 위한 방법.
feature transformaton: 비선형 관계의 데이터를 선현회귀 모델에 적용하기 위해 적용. built in feature가 가장 쉽게 활용. 입력을 벡터로 변환

Elastic net: 리지, 라쏘를 조합해서 사용.
early stopping: 학습 곡선을 관찰하는 중에 validation error가 minimum에 도달하면 학습을 중단.

logistic regression: 이진 분류에 사용되는 모델.

----------------------1118-1 ---------------------------
인터프리터블: 설명가능한
gini impurity: 순수한지 only one class
entropy: 안정적인지
-regularization hypeparameters: 트리 구조는 과적합의 위험 존재.
nonparametric model: 파라미터의 수가 미리 결정된 것이 아니라 데이터에 따라서 결정.
- prunning: 통계적 테스트에 따라 node를 잘라냄

----------------- 1118-2 ----------------
wisdom of the crowd: 한 명의 전문가보다 다수의 랜덤 사람들의 의견이 나을 수있음.
ensemble: 다양성을 주기 위해 여러 모델 이용
hard voring classifier: =majority vote. 가장 많은 예측기가 분류한 클래스를 선택. 매우 많은 수의 weak learner로 구성된 앙상블을 구성하고 learner가 다양성을 확보하면 정확도가 우수해짐.
soft voting: 모든 판별기가 확률을 출력하고 그 평균을 계산해서 가장 높은 확률값을 갖는 클래스를 출력.
분산을 유지하면서 데이터 수를 늘리면 과학습을 방지.
bagging(bootstrap aggregating): pasting이 좋을 수 있지만 bagging을 사용하는 이유는 적은 데이터로 재활용할 weak learner를 더 많이 확보가능하고 다양성이 너무 과하면 기본적으로 공통적인 데이터를 사용하기 위함.
sampling withe replacement
pasting: smapling without replacement(일회용)

bagging에서 랜덤 샘풀링뿐만아니라 feature도 램던하게 선택
random patches method: 데이터와 feature 모두 랜덤하기 추출
random subspace method: 모든 학습 데이터를 사용하지만 feature는 랜덤하게 선택(데이터 수가 적어서 사용)

랜덤성을 주는 이유는 다양성을 주기 위함.
random forest: decision tree의 앙상블로 bagging을 사용하여 학습.
노드 분할시 가장 좋은 feature가 아닌 랜덤 feature 집합에서 가장 좋은 featrue를 기준으로 한다.
루트노드와의 노드의 깊이로 중요도를 확인

boosting: 앙상블 기법의 하나. bagging과 다르게 순차적으로 학습(weak learner의 학습이 끝나면 다은 weak learner는 이전 잘못을 바로 잡는 방향으로 학습이 진행)
adaboost(adaptive boosting): 실패한 데이터에 더 집중
gradientboost: 이전 단계에서학습한 판별기의 residual error를 보정하는 형태로 현대 단계의 판별기를 학습.






