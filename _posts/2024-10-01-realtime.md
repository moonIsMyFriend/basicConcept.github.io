---
layout: post
read_time: true
show_date: true
title:  Like Human
date:   2024-10-01
description: Planning
img: posts/20210324/starting_adventure.jpg
tags: [RL]
author: 
github:  
mathjax: yes
---

## 1. MCTS(Monte Carlo Tree Search)

### 1.1. 실시간 플래닝 알고리즘
주어진 상황에 특화된 해를 찾는데 쓰이는 플래닝 알고리즘으로 그냥 많이 시뮬레이션 해 보고 가장 좋았던 액션을 선택하는 방법. 따라서 다른 상황에서는 재사용이 어렵다.

### 1.2. 학습 단계  
(1) 지도학습 정책 $\pi_{sl}$  
- 학습에 필요한 데이터를 사람의 지식을 이용한 featrue를 인풋으로 학습
- 분류 네트워크를 이용하여 각 자리의 확률을 리턴
- 컨볼루션 레이어: 입력에 공간적인 정보가 담겨있는 경우 이를 표현하기 위해 여러 개의 필터를 통해 정보를 인코딩하는 방식

(2) 롤아웃 정책 $\pi_{roll}$  
- $\pi_{sl}$의 가벼운 버전(선형 결합 레이어를 이용하여 계산이 빠름. 수 많은 시뮬레이션 생성에 이용하기 위함.)  
- 현 상태를 인풋으로 받아서 각 액션의 확률 분포를 리턴

(3) 스스로 강화하는 학습 정책 $\pi_{rl}$
- $\pi_{sl}$와 초기에는 동일하지만 self-play를 통해 계속 강화됨.
- 직접 이용하기 보다는 $v_{rl}$ 를 구하는 역할.

> Matering the real time strategy game StarCraft (2019)

(4) 밸류 네트워크 $v_{rl}$  
- 아웃풋이 1개의 값
- 승자를 예측하는 함수 학습

### 1.3. MCTS

> Mastering the game of Go with deep neural networks and tree search (2016)

#### 1.3.1. 사용하기 위한 조건
(1)  MDP 모델을 알아야 한다.  
(2) 액션 선택 중간마다 시간적 틈이 필요.

#### 1.3.2. 진행 단계
(1) 선택  
- 루트 노드에서 리프 노드까지 가는 과정
- 액션의 밸류가 높은 액션을 선택하도록 기준으로 Q+u 를 사용.  
Q: 시뮬레이션 실행 후 얼마나 좋은지  
u: 시뮬레이션 실행 전에 얼마나 좋을 것인지 추측

(2) 확장   
- 도착한 리프 노드를 실제로 트리에 매달아 주는 과정.  
- 또한 해당 노드에서 뻗어나가는 엣지들의 정보를 초기화.  

(3) 시뮬레이션  
- 노드의 가치를 평가
    - 시뮬레이션으로 바로 평가
    - 또는, 준비해둔 밸류 네트워크 $v_{rl}(s)$를 활용

(4) Back propagation  
- 이미 계산된 밸류의 평균값에 새로운 리프 노드의 밸류를 평균값에 반영하기 위해 평균값 Q(s,a)를 갱신하는 과정
- 결국 이 모든 과정은 루트 노드에 특화된 액션 하나를 선택하는 것이 목적.
- 신뢰도를 고려해서 액션 밸류 Q(s,a) 대신 방문수 즉, N(s,a)를 액션 선택에 이용할 수 있다.
- 물론 가장 높은 액션 하나를 결정론적으로 선택하지 않고 여러 액션별 방문 수에 비례하는 확률 분포를 리턴하도록 해서 stochastic으로 움직이게 할 수 있다. 그러면 MCTS를 마치고 나서 실제 선택에서 매번 다른 액션이 취해질 수 있다.

### 1.4. Zero Human Knowledge
- 학습 때도 MCTS를 활용.

> Mastering the Game of Go without Humman Knowledge (2017)


-----------------------
## 2. 도전 과제  
> Creating Pro-Level AI for Real -Time Fighting Game with Deep Reinforcement Learning 

1. 거대한 공간
2. 실시간 게임
3. 물고 물리는 스킬 관계
4. 다양한 스타일

### 강화학습 적용
1. MDP 모델링: 관측치 정의, 액션 정의, 보상 정의
2. Worker
- 각각의 에이전트가 중앙의 모델DB에서 최신 모델을 받아와서 학습.
- 상대역 에이전트는 과거 모델 중 하나를 랜덤하게 선택.
- 경기가 끝나면 로그 파일을 중앙의 리플레이 버퍼 서버에 업로드.

3. Learner
- 리플레이 버퍼에서 로그를 가져와서 정책 네트워크를 갱신.


### A3C(Asynchronous Advantage Actor-Critic)
- on-policy 알고리즘(경험을 쌓는 네트워크와 학습을 받는 네트워크가 동일)

> Asynchronous methods for deep reinforcement learning (2016)


### ACER(Actor-Critic with Experience Replay)
- A3C에 off-policy 요소를 부여하기 위해 importance sampling 기법을 사용.
- 학습 대상이 되는 네트워크: 스킬을 결정하는 네트워크, 이동과 타깃팅을 결정하는 네트워크, 스킬 액션의 가치를 평가하는 네트워크, 이동과 타깃의 가치를 평가하는 네트워크
- 이동 액셔이 상태 변화에 비치는 영향이 미비하기 때문에 여러개를 묶어서 하나의 액션으로 생각.

> Sample efficient actor-critic with experience replay (2016)

### 다양한 스타일의 에이전트 육성
- 보상 조절 방법(reward shaping) 등을 이용하여 self-play 커리큘럼 학습.
- 핸드 튜닝과정이 필요.

> Superhuman AI for multiplayer poker (2019)  

> Matering the real time strategy game StarCraft (2019)

### PPO(proximal policy optimization, 근접 정책 최적화)
안정적인 학습이 되도록 정책을 점진적으로 업데이트.
목적함수 그래디언트의 분산이 커지지는 문제 해결.

### DDPG(deep deterministic policy gradient, 심층 확정적 정책 그래디언트)
연속공간일 경우 확률밀도함수를 계산하지 않고 행동 그 자체를 계산하고 심층 신경망을 이용하는 방법

